{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'tr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-406e0a3c5e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;31m#     saver = tf.train.Saver()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-406e0a3c5e72>\u001b[0m in \u001b[0;36minputs\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                                                                                                           \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                                                                                                           \u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                                                                                                           [''],[0.0],[''],['']])\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mis_first_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mis_seconde_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_float\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-36-406e0a3c5e72>\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(batch_size, file_name, record_defaults)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mfilename_queue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_input_producer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__file__'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextLineReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_header_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'tr' is not defined"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "\n",
    "def read_csv(batch_size, file_name, record_defaults):\n",
    "    filename_queue = tr.train.string_input_producer([os.path.dirname('__file__') + '/' + file_name])\n",
    "    reader = tf.TextLineReader(skip_header_lines=1)\n",
    "    key, value = reader.read(file_name)\n",
    "    decoded = tf.decode_csv(value, record_defaults=redcor_defaults)\n",
    "    return tf.train.shuffle_batchu(decoded, batch_size=batch_size, capacity=batch_size * 50, min_after_dequeue=batch_size)\n",
    "\n",
    "def inference(X):\n",
    "    # 计算推断模型在数据X上的输出，并将结果返回\n",
    "#     return tf.matmul(X, W) + b\n",
    "    return tf.nn.softmax(combine_inputs(X))\n",
    "\n",
    "def loss(X, Y):\n",
    "    # 依据训练数据X及其期望输出Y计算损失\n",
    "#     Y_predicted = inference(X)\n",
    "#     return tf.reduce_sum(tf.squared_difference(Y, Y_predicted))\n",
    "    return tf.reduce_mean(tf.nn.spare_softmax_cross_entropy_with_logits(combine_inputs(X, Y)))\n",
    "\n",
    "def inputs():\n",
    "    # 读取或生成训练数据X及其期望输出Y\n",
    "#     weight_age = [[84, 46], [73, 20], [65, 52], [70, 30], [76, 57], [69, 25], [63,28], [72, 36], [79, 57],[75, 44],\n",
    "#                  [27, 24], [89, 31], [65, 52], [57, 23], [59, 60], [69, 48], [60, 34], [79, 51], [75, 50], [82, 34],\n",
    "#                  [59, 46], [67, 23], [85, 37], [55, 40], [63, 30]]\n",
    "#     blood_fat_content = [354, 190, 405, 263, 451, 302, 288, 385, 402, 365, 209, 290, 346, 254, 395, 434, 220, 374, 308,\n",
    "#                         220, 311, 181, 274, 303, 244]\n",
    "    \n",
    "#     return tf.to_float(weight_age), tf.to_float(blood_fat_content)\n",
    "    passenger_id, survived, pclass, name, sex, age, sibsp, parch, ticket, fare, cabin, embarked = read_csv(100, 'train.csv',\n",
    "                                                                                                          [[0.0], [0.0],\n",
    "                                                                                                          [0], [''],[''],\n",
    "                                                                                                          [0.0],[0.0],[0.0],\n",
    "                                                                                                          [''],[0.0],[''],['']])\n",
    "    is_first_class = tf.to_float(tf.equal(pclass, [1]))\n",
    "    is_seconde_class = tf.to_float(tf.equal(pclass, [2]))\n",
    "    is_third_class = tf.to_float(tf.equal(pclass, [3]))\n",
    "    gender = tf.to_float(tf.equal(sex, ['female']))\n",
    "    # 最终将所有的特征排列在一个矩阵中，然后对改矩阵转置， 使其每行对应一个样本， 每列对应一种特征\n",
    "    features = tf.transpose(tf.pack([is_first_class, is_seconde_class, is_third_class, gender, age]))\n",
    "    survived = tf.reshape(survived, [100, 1])\n",
    "    return features, survived\n",
    "\n",
    "def train(total_loss):\n",
    "    # 依据计算的总损失训练或调整模型参数\n",
    "    learning_rate = 0.01\n",
    "    return tf.train.GradientDescentOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "def evaluate(sess, X, Y):\n",
    "    # 对训练得到的模型进行评估\n",
    "#     print(sess.run(inference([[80., 25.]])))\n",
    "#     print(sess.run(inference([[65., 25.]])))\n",
    "    predicted = tf.cast(inference(X) > 0.5, tf.float32)\n",
    "    print(sess.run(tf.reduce_mean(tf.cast(tf.equal(predicted, Y), tf.float32))))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    W = tf.Variable(tf.zeros([4, 3]), name='weights')\n",
    "    b = tf.Variable(tf.zeros([3]), name='bias')\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    X, Y = inputs()\n",
    "    \n",
    "    total_loss = loss(X, Y)\n",
    "    train_op = train(total_loss)\n",
    "    \n",
    "    coord = tf.train.Coordinator()\n",
    "    threads = tf.train.start_queue_runners(sess=sess,coord=coord)\n",
    "    \n",
    "    initial_step = 0\n",
    "    ckpt = tf.train.get_checkpoint_state(os.path.dirname('__file__'))\n",
    "    if ckpt and ckpt.model_checkpoint_path:\n",
    "        saver.restore(sess, chpt.model_checkpoint_path)\n",
    "        initial_step = int(ckpt.model_checkpoint_path.rsplit('-', 1)[1])\n",
    "    \n",
    "    # 实际的训练迭代次数\n",
    "    train_steps = 1000\n",
    "    for step in range(train_steps):\n",
    "        sess.run(train_op)\n",
    "        # 出于调试的目的，查看损失值\n",
    "        if step % 10 == 0:\n",
    "            print(\"loss:\", sess.run([total_loss]))\n",
    "#             saver.save(sess, 'my-model', global_step=step)\n",
    "    \n",
    "    evaluate(sess, X, Y)\n",
    "    \n",
    "    coord.request_stop()\n",
    "    coord.join(threads)\n",
    "    \n",
    "#     saver.save(sess, 'my-model', global_step=train_steps)\n",
    "    sess.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
